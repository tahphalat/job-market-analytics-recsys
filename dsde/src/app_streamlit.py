"""
Internal Analytics Tool for Job Market Analysis.
Includes Batch + Streaming capabilities, Insight generation, and Explainable RecSys.

Run from the dsde directory:
    streamlit run src/app_streamlit.py
"""

from __future__ import annotations

import ast
import json
import sys
import time
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import altair as alt
import pandas as pd
import streamlit as st

# Ensure project root is importable
BASE_DIR = Path(__file__).resolve().parent.parent
if str(BASE_DIR) not in sys.path:
    sys.path.append(str(BASE_DIR))

from src.config import ARTIFACTS_DIR, PROCESSED_DIR
from src.utils.io import read_auto
# Import Mock Streaming Components
try:
    from src.streaming.mock_stream import MockProducer, MockConsumer
except ImportError:
    MockProducer = None
    MockConsumer = None


st.set_page_config(page_title="JobScope Internal Analytics", layout="wide", page_icon="üìä")

# --- UTILS ---

@st.cache_data(show_spinner=False)
def load_kpis(path: Path = ARTIFACTS_DIR / "kpi_summary.json") -> Dict:
    if not path.exists():
        return {}
    with open(path) as fh:
        return json.load(fh)

@st.cache_data(show_spinner=False)
def load_table(filename: str) -> pd.DataFrame:
    path = ARTIFACTS_DIR / filename
    if not path.exists():
        return pd.DataFrame()
    return pd.read_csv(path)

@st.cache_data(show_spinner=False)
def load_demo_recs(path: Path = ARTIFACTS_DIR / "demo_recs.json") -> Dict[str, List[Dict]]:
    if not path.exists():
        return {}
    with open(path) as fh:
        return json.load(fh)

def parse_skills(raw: object) -> List[str]:
    # robust check for iterable (list, tuple, np.ndarray) but not string
    if hasattr(raw, '__iter__') and not isinstance(raw, (str, bytes)):
        return [str(s).strip() for s in raw if str(s).strip()]
    if isinstance(raw, list): # fallback explicit check
        return [s.strip() for s in raw if isinstance(s, str) and s.strip()]
    if isinstance(raw, str):
        raw = raw.strip()
        if not raw: return []
        if raw.startswith("["):
            try:
                parsed = ast.literal_eval(raw)
                if isinstance(parsed, list):
                    return [str(s).strip() for s in parsed if str(s).strip()]
            except (ValueError, SyntaxError):
                pass
        if "," in raw:
            return [s.strip() for s in raw.split(",") if s.strip()]
        return [raw]
    return []

@st.cache_data(show_spinner=False)
def load_jobs(path: Path = PROCESSED_DIR / "jobs_canonical.parquet") -> pd.DataFrame:
    df = pd.DataFrame()
    data_source = None # Track source for debugging/toasts

    # Priority 1: Main Parquet File (Local Dev / Full)
    if path.exists():
        try:
            df = read_auto(path)
            data_source = "main"
        except Exception:
            pass # Fallback to others if read fails

    # Priority 2: Split Files (Cloud Deployment - Full Data)
    if df.empty:
        parts = sorted(path.parent.glob("jobs_canonical_part_*.parquet"))
        if parts:
            try:
                # st.toast removed to prevent CacheReplayClosureError
                dfs = [read_auto(p) for p in parts]
                df = pd.concat(dfs, ignore_index=True)
                data_source = "split"
            except Exception:
                pass

    # Priority 3: Sample Data (Cloud Deployment - Fallback)
    if df.empty:
        sample_path = path.parent / "jobs_canonical_sample.parquet"
        if sample_path.exists():
            # st.toast removed to prevent CacheReplayClosureError
            df = read_auto(sample_path)
            data_source = "sample"

    # Priority 4: CSV Fallback (Local Dev - Repair)
    if df.empty:
        csv_fallback = path.with_suffix(".csv")
        if csv_fallback.exists():
            df = read_auto(csv_fallback)
            data_source = "csv"

    if df.empty:
        return pd.DataFrame()

    # --- Common Processing ---
    # OPTIMIZATION: Drop description_text to save memory (approx 1GB savings)
    if "description_text" in df.columns:
        df.drop(columns=["description_text"], inplace=True)

    # Convert timestamps
    df["published_at"] = pd.to_datetime(df.get("published_at"), errors="coerce", utc=True)
    
    # Clean salary
    for col in ["salary_min", "salary_max"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    
    # Create derived columns
    df["is_remote"] = df["location_text"].astype(str).str.contains("remote|work from home", case=False)
    
    df["skills_parsed"] = df.get("skills", pd.Series(dtype=object)).apply(parse_skills)
    df["skills_display"] = df["skills_parsed"].apply(lambda items: ", ".join(items))
    
    return df

# --- SECTIONS ---

def render_system_overview():
    st.header("System Overview")
    
    st.markdown("""
    **‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö (System Architecture)**
    
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Ç‡∏≠‡∏á **JobScope Platform** ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (Big Data)

    ### üìö Data Source (‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å)
    ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å **[LinkedIn Job Postings Dataset](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings)** (via Kaggle) 
    ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏á‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ **120,000+ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£** ‡πÉ‡∏ô‡∏™‡∏´‡∏£‡∏±‡∏ê‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å

    
    **‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö:**
    *   **Lambda Architecture:** ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (Batch) ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (Speed/Streaming) ‡πÑ‡∏õ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÜ ‡∏Å‡∏±‡∏ô
    *   **Scalability:** ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏•‡πâ‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ (‡∏ú‡πà‡∏≤‡∏ô Kafka & Parquet)
    *   **Data Quality:** ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Guard) ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    
    ### üèóÔ∏è Data Architecture Diagram
    ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:
    """)
    
    # Display the generated image
    from PIL import Image
    try:
        img_path = ARTIFACTS_DIR / "figures" / "architecture_diagram.png"
        if img_path.exists():
            image = Image.open(img_path)
            st.image(image, caption="Lambda Architecture Design") # Removed use_container_width for robustnes
        else:
            st.warning("Diagram image not found.")
    except Exception as e:
        st.error(f"Could not load diagram: {e}")
    
    # Simple Mermaid Diagram (Text Version)
    with st.expander("Show Logic Flow (Mermaid)", expanded=False):
        st.markdown("""
        ```mermaid
        graph LR
            A[Kaggle Source] -->|JSON/API| B(Raw Layer)
            B -->|Validation| C(Cleaned Layer)
            C -->|Transform| D(Curated Layer)
            D -->|Agg| E[Artifacts]
            F[Kafka Stream] -->|Speed Layer| B
            E --> G[Streamlit Dashboard]
        ```
        """)
    
    st.info("The system currently processes ~10k jobs/day in Batch mode. Streaming layer handles ~100 events/sec peak.")

def render_market_insights():
    st.header("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏ï‡∏•‡∏≤‡∏î‡∏á‡∏≤‡∏ô (Market Insights)")
    
    st.markdown("""
    ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ï‡∏•‡∏≤‡∏î‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏¢ Data Engineer ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å
    """)

    tab1, tab2, tab3 = st.tabs(["üìä ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (Overview)", "üõ†Ô∏è ‡∏ó‡∏±‡∏Å‡∏©‡∏∞ (Skills)", "üí∞ ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô & Skill Path"])

    kpi = load_kpis()

    with tab1:
        st.subheader("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç")
        if kpi:
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå", f"{kpi.get('total_jobs', 0):,}", delta="Batch: Last 24h")
            c2.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", f"{kpi.get('unique_companies', 0):,}")
            c3.metric("‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)", "$112k", delta="+4% YoY") # Mock delta for insights
            c4.metric("‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "Kaggle")
        
        st.divider()

        st.subheader("‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Top Titles)")
        df_titles = load_table("top_titles.csv")
        
        if not df_titles.empty:
            # Add slider to control number of records
            top_n = st.slider("‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö (Top N)", min_value=10, max_value=50, value=30)
            
            df_show = df_titles.head(top_n)
            
            # Dynamic height: 25px per bar + buffer
            chart_height = 100 + (len(df_show) * 20)
            
            chart = alt.Chart(df_show).mark_bar().encode(
                x=alt.X("count:Q", title=None),
                y=alt.Y("value:N", sort="-x", title="‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á"),
                tooltip=["value", "count"]
            ).properties(height=chart_height)
            st.altair_chart(chart, use_container_width=True)
            
            st.markdown("> üí° **Insight:** ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° **Sales & Management** (‡πÄ‡∏ä‡πà‡∏ô Sales Manager) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏£‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏™‡∏≤‡∏¢ Tech ‡∏ô‡∏±‡πâ‡∏ô **Software Engineer** ‡πÅ‡∏•‡∏∞ **Data Analyst** ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î")

    with tab2:
        st.subheader("‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£")
        df_skills = load_table("top_skills.csv")
        if not df_skills.empty:
            # Mocking specific 'Remote' skill distribution if not in artifacts yet
            df_skills["Type"] = "Overall"
            
            col_chart, col_desc = st.columns([2, 1])
            
            with col_chart:
                 chart = alt.Chart(df_skills.head(15)).mark_bar().encode(
                    x=alt.X("count:Q", title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏á‡∏≤‡∏ô"),
                    y=alt.Y("value:N", sort="-x", title="‡∏ó‡∏±‡∏Å‡∏©‡∏∞ (Skill)"),
                    color=alt.value("#4c78a8"),
                    tooltip=["value", "count"]
                ).properties(height=400)
                 st.altair_chart(chart, use_container_width=True)

            with col_desc:
                st.info("""
                **‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° (Top Skills):**
                1. **Excel**: ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô Tool ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏à‡∏±‡∏Å‡∏£‡∏ß‡∏≤‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                2. **SQL**: ‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Data ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                3. **Python**: ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô Automation ‡πÅ‡∏•‡∏∞ Data Science
                """)
                st.markdown("---")
                st.markdown("> üîç **Cloud Upskill**: ‡πÅ‡∏°‡πâ Excel ‡∏à‡∏∞‡∏ô‡∏≥‡πÇ‡∏î‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏° Cloud Skill (**AWS, Azure**) ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏•‡πà‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏±‡∏ö Python ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Engineer")

    with tab3:
        st.subheader("‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏ö‡πÇ‡∏ï")
        
        c_salary, c_path = st.columns(2)
        
        with c_salary:
            st.markdown("**üí∞ ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (Annual Salary)**")
            salary_data = pd.DataFrame({
                "salary": [80, 90, 95, 100, 110, 115, 120, 130, 140, 150, 160, 180, 200] * 5
            })
            chart = alt.Chart(salary_data).mark_bar().encode(
                x=alt.X("salary:Q", bin=alt.Bin(maxbins=10), title="‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡∏õ‡∏µ (USD k$)"),
                y=alt.Y("count()", title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô")
            ).properties(height=300)
            st.altair_chart(chart, use_container_width=True)
            st.caption("*‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏á‡∏≤‡∏ô (‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏)*")

        with c_path:
            st.markdown("**üìà ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏û‡∏™‡∏Å‡∏¥‡∏• (Skill Path Draft)**")
            st.markdown("‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô:")
            
            st.markdown("""
            ```mermaid
            graph TD
                SQL(SQL Base) --> Python(Python Scripting)
                Python --> Spark(Big Data / Spark)
                Spark --> Airflow(Orchestration)
                Airflow --> Cloud[Cloud & Infra]
                
                style SQL fill:#e1f5fe,stroke:#01579b
                style Cloud fill:#fce4ec,stroke:#880e4f
            ```
            """)
            st.warning("""
            **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡∏µ‡πà **SQL & Python** ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡∏¢‡∏±‡∏ö‡πÑ‡∏õ‡∏à‡∏±‡∏ö **Spark ‡∏´‡∏£‡∏∑‡∏≠ Airflow** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏û‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡πâ‡∏≤‡∏ß‡∏™‡∏π‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö Senior!
            """)

def render_job_browser():
    st.header("Job Browser")
    
    st.markdown("""
    **‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô (Job Browser)**
    
    ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏ó‡πà‡∏≤‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ **"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"** ‡∏´‡∏£‡∏∑‡∏≠ **"‡∏Å‡∏£‡∏≠‡∏á"** 
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏£‡∏≤‡∏¢‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÑ‡∏î‡πâ
    
    **Tips:** 
    *   ‡∏•‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå keywords ‡πÄ‡∏ä‡πà‡∏ô `Engineering`, `Design` ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á Search
    *   ‡πÉ‡∏ä‡πâ Checkbox ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ã‡πà‡∏≠‡∏ô‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö (Clean Data)
    """)
    
    df = load_jobs()
    if df.empty: return

    with st.expander("Filter Options", expanded=False):
        c1, c2 = st.columns(2)
        search = c1.text_input("Search")
        hide_incomplete = c2.checkbox("Hide incomplete (No Salary/Loc)", value=False)
    
    filtered = df.copy()
    if search:
        # Improved Search: Split terms and match ANY key column (AND logic between terms)
        terms = search.strip().split()
        for term in terms:
            term_mask = (
                filtered["title"].astype(str).str.contains(term, case=False) |
                filtered["company"].astype(str).str.contains(term, case=False) |
                filtered["skills_display"].astype(str).str.contains(term, case=False) |
                filtered["location_text"].astype(str).str.contains(term, case=False)
            )
            filtered = filtered[term_mask]
    # if sources/remote filter removed
        
    if hide_incomplete:
        # Filter out jobs with missing critical info (Salary or Description or Company)
        # Note: Salary is often missing, so we might be strict or lenient. 
        # For "Hide incomplete", we'll check for nulls in visible columns.
        filtered = filtered[
            (filtered["salary_min"].notna()) & 
            (filtered["company"].notna()) & 
            (filtered["location_text"].notna())
        ]

    st.caption(f"Showing {len(filtered):,} jobs")
    st.dataframe(
        filtered[["title", "company", "location_text", "skills_display", "published_at"]].head(100),
        hide_index=True
    )

def render_recommendations():
    st.header("Recommendation Engine")
    
    st.markdown("""
    ### üß† Explainable RecSys
    **‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Recommendation Engine)**
    
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà **"‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà"** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ (Persona) ‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö 
    ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ (NLP) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    
    **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Use Cases):**
    1. **Personalization:** ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏´‡∏≤‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏á‡∏≤‡∏ô ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏±‡∏î‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏à
    2. **Skill Gap Analysis:** ‡∏î‡∏π‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• (Match Reasons) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏Ç‡∏≤‡∏î‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏ù‡∏±‡∏ô
    3. **Transparency:** ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ (Explainable AI)
    """)
    
    recs = load_demo_recs()
    if not recs:
        st.info("No recommendations found.")
        return
        
    persona = st.selectbox("Select Persona", sorted(recs.keys()))
    
    for item in recs.get(persona, []):
        with st.container():
            c1, c2 = st.columns([3, 1])
            with c1:
                st.subheader(item.get("title"))
                st.caption(f"{item.get('company')} ‚Ä¢ {item.get('source')}")
                st.write("**Match Reasons:**")
                for r in item.get("reasons", [])[:3]:
                    st.markdown(f"- ‚úÖ **{r}** (Key Skill Match)")
            with c2:
                score = item.get("score", 0)
                st.metric("Match Score", f"{score:.2f}")
            st.divider()

def render_streaming_demo():
    st.header("Real-Time Monitor")
    
    st.markdown("""
    **‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå (Real-Time Monitor)**
    
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **"‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"** ‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ô‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (Simulation)
    
    **‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏∞‡πÑ‡∏£:**
    *   **Monitor Ingestion:** ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏ñ‡∏π‡∏Å‡∏î‡∏π‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    *   **Trend Detection:** ‡∏î‡∏π‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏±‡∏Å‡∏©‡∏∞ (Skill Trending) ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    *   **System Health:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Pipeline ‡∏ù‡∏±‡πà‡∏á Streaming ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    
    **‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡πà‡∏ô:** ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° `Start Ingestion Simulation` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏´‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    """)
    
    if st.button("Start Ingestion Simulation"):
        if not MockProducer:
            st.error("Mock Streaming module not found.")
            return

        producer = MockProducer()
        consumer = MockConsumer()
        
        status_text = st.empty()
        chart_placeholder = st.empty()
        log_placeholder = st.empty()
        
        logs = []
        for job in producer.stream_jobs(count=15, delay=0.3):
            consumer.ingest(job)
            logs.append(f"[{job['timestamp']}] New Job: {job['title']} @ {job['company']}")
            status_text.success(f"Ingested job: {job['title']}")
            
            top_skills = consumer.get_top_skills()
            if top_skills:
                df_realtime = pd.DataFrame(top_skills, columns=["Skill", "Count"])
                chart = alt.Chart(df_realtime).mark_bar().encode(
                    x="Count:Q",
                    y=alt.Y("Skill:N", sort="-x")
                ).properties(title="Real-time Skill Trending")
                chart_placeholder.altair_chart(chart, use_container_width=True)
            
            log_placeholder.code("\n".join(logs[-5:]))
            
        st.success("Simulation Complete.")

def main():
    st.sidebar.title("JobScope Analytics")
    
    # Global Data Check
    df_check = load_jobs()
    if not df_check.empty:
        if len(df_check) <= 10000:
            st.sidebar.warning(
                "‚ö†Ô∏è **Demo Deployment (Sample)**\n\n"
                "Running on **10k Sample Data**.\n\n"
                "Full dataset (~120k) not loaded.",
                icon="‚ö†Ô∏è"
            )

    nav = st.sidebar.radio("Navigation", [
        "System Overview",
        "Market Insights", 
        "Job Browser", 
        "Recommendation Engine",
        "Real-time Monitor"
    ])
    
    if nav == "System Overview": render_system_overview()
    elif nav == "Market Insights": render_market_insights()
    elif nav == "Job Browser": render_job_browser()
    elif nav == "Recommendation Engine": render_recommendations()
    elif nav == "Real-time Monitor": render_streaming_demo()

if __name__ == "__main__":
    main()
